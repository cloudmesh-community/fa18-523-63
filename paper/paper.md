# scikit-learn

Scikit-learn is Python's inherent machine learning library. It is a robust library that intends uses object oriented programming to implement commonly used machine learning algorithms effectively, efficiently, pythonically, and swiftly. While, for specific purposes, many experts are able to implement their own algorithms that may improve upon the Scikit-learn library, it has sufficient tools and robustness that enable it to be the leading library for machine learning topics within Python.

Many people downplay the need for effective centralized machine learning algorithms as there are many experts in the industry that can hard code sophisticated algorithms from scratch. For the rest of us, the yet-to-become experts, there are built in libraries to Python that can make these tasks much simpler to understand and to implement. Despite these experts, junior machine learning experts are gaining footing in the industry and are able to gain reputation, thanks to the help of many of these libraries.

As data becomes larger and larger with time, experts in the field are needed to perform this operation. Or, better stated, experts are needed to be able to program computers to perform these operations for them. A computer can process streamlined and well-defined data faster than humans in some instances, especially when reviewing the data becomes increasingly tedious. While algorithms may never be as good at recognizing what is in an image quite like a human can, they will become closer and closer to the point where advertizing will be even more targeted, devices will understand the wants of their human masters clearly, and effective decisions can be made with minimal error. Scikit-learn is not the most sophisticated a capable machine learning algorithm out there, but it is effective and easily implemented via Python, which will make it the workhorse of my semester project.

# The supervised algorithms (some of them)

+ Nearest Neighbors: Nearest neighbors is a machine learning algorithm that makes the decision of one input variable based on training data (making it a supervised algorithm) that most similarly matches itself. Once one of the training sets is identified as the closest match of inputs, it will assign the same category for the test instance.
+ Naive Bayes: Naive Bayes takes a look at Baysian statistics and makes one majorly naive assumption: all of the inputs are independent of eachother. While this is a glaring assumption, due to ease of implementations, most issues that would arise from generally erroneous assumption are not impactful. 
+ Decision trees: Separating on different attributes of the input data, decision trees are one of the more robust machine learning algorithms in that they are able to handle a wide variety of inputs and still maintain their quality. Splitting on each attributes (usually on traits that maximize the entropy of the model, to enhance effectiveness). A branch off algorithm to decision trees are random forests which use many small, randomly chosen (with replacement) trees that can use small subsets of the data to formulate better opinions in a less computationally intensive way.
+ Neural network models (supervised) are algorithms that are particularly good at image classification. They obtain different neurons, each of which contributes in the decision making. They are based off of the way a human neural network would work if it could be modelled accurately via code. Eeach neural network has different levels which contribute to the decision making process.

# The unsupervised algorithms (some of them)
+ Clustering: Stemming from the k-means clustering, this is designed to group the datapoints based on similarity to others in in the same dataset. The inputs are generally strictly numerical but can be n-dimmensional. Clustering converges quickly via iterative methods but is highly sensitive to initialization, making it very important to have domain knowledge and valuable visualization strategies when the data is 3-dimmensional and higher.
+ Neural Network (unsupervised): much like the aforementioned neural networks, sklearn has libraries for unsupervised machine learning algorithms, which don't require training data to make decisions. In this sense, it becomes more of a clustering algorithm than a group identification.
# Other Method groupings within sklearn
+ Dataset transformations: Oftentimes, data comes mangled and hard to use, requiring the need for effective data wrangling. scikit-learn has methods for feature extraction, preprocessing, random projection, dimmensionality reduction, and more. This makes it a valuable library for more aspects than just the machine learning algorithms themselves. 
+ Dataset loading utilities: Scikit-learn has the utilities needed to load data as well as read it. There are Application Programming Interfaces for training datasets, real-world datasets, generated-datasets, and the tools needed to use them effectively.
# More to scikit
There are more uses and tools in scikit-learn than what I have mentioned here. I couldn't just copy the user guide or man pages for these articles, even though they are good. The user guide contains valuable examples and assists with syntax. You will need basic machine learning understanding to be able to use any of these methods in this library. Once the understanding is there and basic implementations are used, microtuning and enhancing of the algorithms comes quickly and simply to an expert with a good eye. I didn't mention every individual method because there are plenty that I have no experience using and don't understand the nature of the machine learning or other algorithm. 
# Real world applications for scikit learn
The real world applications are nearly as endless as are the applications for machine learning and artificial intelligence. The trick is getting the data to work together, whether that be through internet of things, internet of computers, internet of people, etc. I have personally used this tool in many ways, ranging from sports analytics to automation of analysis of the stock exchange. Expert knowledge of this library alone can bring six-figure salaries as a machine learning engineer, which many major and minor companies alike choose to employ. 
# An introduction to the project
For my project, I will be using scikit learning heavily. I intend to create several different virtual machiens that will each perform a different machine learning tool on different datasets related to financial analytics. My goal is to improve my current model using cloud computing technologies. The reason behind needing the virtual machines for this is a cosmetic need, for the sake of learning some cloud computing characterstics and intricaces for both my education and my professional career and life. One virtual machine will be used to collect daily data and statistical reports using either Yahoo's or Google's APIs for financial data on the New York Stock Exchange. Other machines will poll this data either of scp, ssh, or some other connection protocol of my choosing, and perform initial analyses, looking for correlation between the different features of the data. This will range from basic statistical analyses of the correlation of the variables to more in depth of finding correlations of the outcomes of the different machine learning algorithms. It will be recursive in nature and test it's estimations across the different days of data. Another machine will be pulling more realtime data using realtime stock, a python library for just this purpose. This would be ideal for day-traders and and more immediate decision making. A third machine will be webscraping for relevant news on a stock market ticker and its competitors to see whether that has any impact on the outcome. One machine will be running a SQL database storing the data collected by the other machines. Lastly, there will be a machine dedicated to each different machine learning algorithm, as there will be a lot of big data to sort and sift through.
# Hardware needed
I will utilize tools such as virtual box to implement this. I have a personal computer with the spefications that I believe will be able to handle the daily load in approximately 4 hours of runtime (but time will tell). The virtual machine that will need the most disk space will be the one running the SQL database. The others will need high amounts of memory, which should not be an issue for my PC that has 32 GB of internal memory with 500 GB of flash storage space if it is insufficient.
# Software needed
I will likely be running my implementations at night, reducing the need for monitoring. Scikit will be used heavily in each virtual machine as that is the major workhorse for the analytics of the project. I will write code independantally on linux (likely Xubuntu boxes) which should give ease of reproduction and implementation.
# Expected challenges
There are many expected challenges and I anticipate that I underestimate the difficulty of many aspects of this project. Firstly, I don't have a storage area network designed specifically for my data needs. I do not expect to exceed 500 GB of data used but if I do then I believe that I have the means to procure the appropriate resources for this project. 
+ Computing resouces: I haven't yet done thorough enough analyses to understand all of the CPU/GPU power that will be required for this project. I do have a healthy and stable CPU which I believe will help, but time will tell whether it will be sufficient for my needs.
+ Memory resources. I don't expect to exceed the 32 GB of memory that I have procured and I do have a healthy amount of flash storage which should serve as a capable backup, resulting in minimal overhead when it comes to computing time. I will only be using spinning disks for the SQL database but I have not yet decided whether I will make this RAID protected or not.
+ Storage resources: as mentioned earlier, I will not have access to a full storage area network and the potential exists for exceeding the disk capactiy that I have available for this project. This is a minimal fear as prelimary tests didn't use more than 100 GB of data, which was easily handled byh one of my solids state drives.
+ I anticipate that I understimate the difficulty of the communcation between the different virtual machines. I am doing it in the way that I am to make this a learning opportunity. I know that this will add overhead but it is expected to be joyous overhead designed with the purposes of learning the domain private cloud computing.
+ Implemenation of the machine learning algorithms will be a little difficult as I only consider myself an expert in about 4 different algorithms, the ones that I know how to code from scratch. For ther others, I will be using scikit-learn for that express purpose, to ease the implemenation.
+ I expect that I will run into some networking issues despite it all being hosted on a singular machine. I will need to carefully define IP addresses or devise names to mitigate this communication issue and ensure that all of the proper TCP ports will be open for the communication to occur between my storage nodes, my analytic nodes, and my results nodes.
# Project Hypotheses
+ I conject that I will be able to improve my currently used model for my automate robo-advising tools by nearly 30% of the growth. This modest prediction is likely to occur because I will be using more sophisticated machine learning tools and I will not be overutilizing a single program at once (as multithreaded as my previous code may be). 
+ I hypothesize that this method will have 40% less overhead than my previous method as multiple algorithms will be able to run simultaneously and the different algorithms will be independent of eachother.
# The end goal
The end goal will be a final machine learning clustering algorithm to assign to one of five categories for each stock type: strong sell, weak sell, neutral, weak buy, strong buy. The only stocks that I intend to own are those that will result in the strong buy category and I intend to refuse to own any that show up in the strong sell category. I believe that this heavy use of machine learning will prove to be an effective improvement on my currently used models and will result in the model that I will use in the future, minus many of the overheads of the virtual machines.
